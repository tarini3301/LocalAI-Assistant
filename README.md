#  LocalAI-Assistant

**Private, Offline AI Chatbot powered by Ollama + Streamlit + Whisper + LLaVA + RAG**

![badge](https://img.shields.io/badge/Offline-AI-blue) ![badge](https://img.shields.io/badge/Privacy-100%25-green) ![badge](https://img.shields.io/badge/Framework-Streamlit-orange)

---

##  Overview

**LocalAI-Assistant** is a fully private, offline AI-powered assistant that runs on your local machine. It supports multiple AI models like **Llama, Mistral, DeepSeek, Phi, TinyLlama**, and offers features such as:

-  **Chat with LLMs (Offline)**
-  **PDF Summarization**
-  **Voice Input & Output (Speech-to-Text & Text-to-Speech)**
-  **Image Analysis with LLaVA**
-  **Chat with Documents using RAG (Retrieval-Augmented Generation)**
-  **Multi-Chat Memory (Auto Save) with Rename, Delete, Recycle Bin with Restore**
-  **No Internet Required. 100% Local & Private**

---

##  Tech Stack

-  **Ollama** ‚Äî Run LLMs like Llama3, Mistral, DeepSeek, Phi, TinyLlama, LLaVA
-  **Streamlit** ‚Äî For the web-based UI
-  **LangChain** ‚Äî For Document RAG (Chat with Docs)
-  **Whisper** ‚Äî Speech to Text (Offline)
-  **pyttsx3** ‚Äî Text to Speech (Offline)
-  **LLaVA** ‚Äî Vision-based LLM for Image Analysis
-  **JSON-based Local Storage** ‚Äî For chat memory persistence

---

##  Installation

### 1Ô∏è‚É£ Install Ollama

- Download and install Ollama from üëâ https://ollama.com/download

- Run Ollama in the background:
```bash
ollama serve
```
### 2Ô∏è‚É£ Clone the Repository:
```bash
git clone https://github.com/your-username/LocalAI-Assistant.git
cd LocalAI-Assistant
```


### 3Ô∏è‚É£ Install Python Dependencies
Create a virtual environment (recommended):

```bash
python -m venv venv
# Activate it:
venv\Scripts\activate  # On Windows
source venv/bin/activate  # On Mac/Linux
```

```bash
pip install -r requirements.txt
```

### 4Ô∏è‚É£ Pull Models via Ollama:
```bash
ollama pull llama3
ollama pull mistral
ollama pull deepseek-coder
ollama pull phi3
ollama pull tinyllama
ollama pull llava
```

### ‚ñ∂Ô∏è How to Run
Run the app:
```bash
streamlit run app.py
```
Open in browser:
```bash
http://localhost:8501
```
